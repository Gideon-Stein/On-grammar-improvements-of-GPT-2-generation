{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Lambada on the new models and compare\n",
    "- This should give us an idea whether the finetuned models loose LM capacities\n",
    "\n",
    "\n",
    "\n",
    "Results: All works reasonably well. However the LM cappacities are slightly declining.\n",
    "\n",
    "- 0.355 on the test set\n",
    "\n",
    "- 0.325 on the test set when finetuned 1 Epoch\n",
    "- 0.326 on the test set when finetuned 20 Epochs\n",
    "- 0.3214 on test set using the full translation model as a generator trained 20 Epochs\n",
    "- 0.3302 on test set using the 6layer translation model as a generator trained 20 Epochs\n",
    "- 0.3194 on test set using the full translation model as a generator trained 10 EPochs\n",
    "\n",
    "\n",
    "- 0.342 on test set using the full translation model trained for 20 Epochs  + baseline\n",
    "- 0.314 on test set using the full translation model  trained for 20 Epochs + finetune 1 Epoch\n",
    "- 0.3088 on test set using the full translation model trained for 20 Epochs as a generator and translator\n",
    "\n",
    "- 0.344 on test set using the 6layer translation model trained for 20 Epochs  + baseline\n",
    "- 0.3156 on test set using the 6layer translation model  trained for 20 Epochs + finetune 1 Epoch\n",
    "- 0.3194 on test set using the 6layer translation model trained for 20 Epochs as a generator and translator\n",
    "\n",
    "\n",
    "- 10 Epochs has to be worse in all cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = open(\"LAMBADA/lambada_control_test_data_plain_text.txt\",\"r+\",encoding=\"utf-8\")\n",
    "lam = lam.read()\n",
    "lam = lam.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from utility import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuned(path):\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.load_state_dict(torch.load(\"../transformers/examples/model_save/\" + str(path)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lamT = []\n",
    "for x in lam:\n",
    "    lamT.append(tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a917d4af28fb4f35a4fee5926f624fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(GPT_org(torch.tensor(x[:-1]))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3548\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal finetune\n",
    "\n",
    "- 20 Epochs\n",
    "- 1 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_classic_finetune/checkpoint-41599/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(model(torch.tensor(x[:-1]).to(\"cuda\"))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds_fine_1.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_classic_finetune/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(model(torch.tensor(x[:-1]).to(\"cuda\"))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds_fine_20.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds_fine_1.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.325\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds_fine_20.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3264\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation model without translation (with 20/10 epochs 12 /6 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream/TheDream10EPochs/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(model(torch.tensor(x[:-1]).to(\"cuda\"))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds_trans_no_trans_10.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds_trans_no_trans_10.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3194\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation model fully trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"The Technology Report empowers or enlightens. ==== The Technology Report empowers or enlightens.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With base sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pickle.load(open(\"saves/save_preds.p\",\"rb\"))\n",
    "step1_corpus = lamT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out= []\n",
    "for x in range(len(step1)):\n",
    "    final = step1[x][0]\n",
    "    sentence = tokenizer.decode(lamT[x][:-1])\n",
    "    out.append(sentence + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = split_eos(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "for x in eos: \n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_trans.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pickle.load(open(\"saves/save_preds_trans.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in prediction:\n",
    "    out.append(x.replace(\"<|endoftext|>\",\"\"))\n",
    "prediction = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred = []\n",
    "for x in prediction: \n",
    "    pred.append(tokenizer.decode(tokenizer.encode(x)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3418\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in range(len(pred)):\n",
    "    if pred[x] == real[x][1]:\n",
    "        counter+=1\n",
    "print(counter/len(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_final_6/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "for x in eos:\n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_trans_6.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pickle.load(open(\"saves/save_preds_trans_6.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in prediction:\n",
    "    out.append(x.replace(\"<|endoftext|>\",\"\"))\n",
    "prediction = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred = []\n",
    "for x in prediction: \n",
    "    pred.append(tokenizer.decode(tokenizer.encode(x)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.344\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in range(len(pred)):\n",
    "    if pred[x] == real[x][1]:\n",
    "        counter+=1\n",
    "print(counter/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With transgenerated sentences (with 20/10/5 epochs) 6 and full epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pickle.load(open(\"saves/save_preds_trans_no_trans_6_20.p\",\"rb\"))\n",
    "step1_corpus = lamT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out= []\n",
    "for x in range(len(step1)):\n",
    "    final = step1[x][0]\n",
    "    sentence = tokenizer.decode(lamT[x][:-1])\n",
    "    out.append(sentence + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = split_eos(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_final_6/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0 \n",
    "for x in eos: \n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_trans_trans_6_20.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pickle.load(open(\"saves/save_preds_trans_trans_6_20.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in prediction:\n",
    "    out.append(x.replace(\"<|endoftext|>\",\"\"))\n",
    "prediction = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred = []\n",
    "for x in prediction: \n",
    "    pred.append(tokenizer.decode(tokenizer.encode(x)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3194\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in range(len(pred)):\n",
    "    if pred[x] == real[x][1]:\n",
    "        counter+=1\n",
    "print(counter/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"The Technology Report empowers or enlightens. ==== The Technology Report empowers or enlightens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pickle.load(open(\"saves/save_preds_fine_1.p\",\"rb\"))\n",
    "step1_corpus = lamT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out= []\n",
    "for x in range(len(step1)):\n",
    "    final = step1[x][0]\n",
    "    sentence = tokenizer.decode(lamT[x][:-1])\n",
    "    out.append(sentence + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = split_eos(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "for x in eos: \n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_combined.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_final_6/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "count = 0\n",
    "for x in eos: \n",
    "    count +=1\n",
    "    if count%100 == 0: \n",
    "        print(count)\n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_combined_6.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pickle.load(open(\"saves/save_preds_combined_6.p\",\"rb\"))\n",
    "real = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in prediction:\n",
    "    out.append(x.replace(\"<|endoftext|>\",\"\"))\n",
    "prediction = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred = []\n",
    "for x in prediction: \n",
    "    pred.append(tokenizer.decode(tokenizer.encode(x)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3156\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in range(len(pred)):\n",
    "    if pred[x] == real[x][1]:\n",
    "        counter+=1\n",
    "print(counter/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are the specific cases were there models diverge? (prediction,real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = []\n",
    "for x in predictions:\n",
    "    real.append(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = pickle.load(open(\"saves/save_preds_fine_1.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions3 = pickle.load(open(\"saves/save_preds_fine_20.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions4 = pickle.load(open(\"saves/save_preds_trans_no_trans_20.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions5 = pickle.load(open(\"saves/save_preds_trans.p\",\"rb\"))\n",
    "tokens = []\n",
    "for x in predictions5:\n",
    "    tokens.append(tokenizer.decode(tokenizer.encode(x)[-2:-1]))\n",
    "predictions5 = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions6 = pickle.load(open(\"saves/save_preds_combined.p\",\"rb\"))\n",
    "tokens = []\n",
    "for x in predictions6:\n",
    "    tokens.append(tokenizer.decode(tokenizer.encode(x)[-2:-1]))\n",
    "predictions6 = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = {}\n",
    "correct[\"base\"] = []\n",
    "correct[\"fine1\"] = []\n",
    "correct[\"fine20\"] = []\n",
    "correct[\"transgen\"] = []\n",
    "correct[\"trans\"] = []\n",
    "correct[\"combo\"] = []\n",
    "for x in range (5000):\n",
    "    if real[x] == predictions[x][0]:\n",
    "        correct[\"base\"].append(x)\n",
    "    if real[x] == predictions2[x][0]:\n",
    "        correct[\"fine1\"].append(x)\n",
    "    if real[x] == predictions3[x][0]:\n",
    "        correct[\"fine20\"].append(x) \n",
    "    if real[x] == predictions4[x][0]:\n",
    "        correct[\"transgen\"].append(x)  \n",
    "    if real[x] == predictions5[x]:\n",
    "        correct[\"trans\"].append(x)\n",
    "    if real[x] == predictions6[x]:\n",
    "        correct[\"combo\"].append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in correct[\"base\"]: \n",
    "    if x not in correct[\"fine1\"]:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' sound', ' sound')"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[2398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(' that', ' sound')"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2[2398]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"there was only one person she knew who might be able to help grant. her hands were shaking when she dialed keith's number. he was the only defense lawyer she knew, and she didn't know where else to turn. `` isabelle, '' he greeted her. she could hear the smile in his voice, and it helped calm her nerves a bit to know he was still safe and sound\""
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(lamT[2398])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
