{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Lambada on the new models and compare\n",
    "- This should give us an idea whether the finetuned models loose LM capacities\n",
    "\n",
    "\n",
    "\n",
    "Results: All works reasonably well. However the LM cappacities are slightly declining.\n",
    "\n",
    "- 0.3548 on the test set\n",
    "- 0.325 on the test set when finetuned 1 Epoch\n",
    "- 0.3264 on the test set when finetuned 20 Epochs\n",
    "- 0.342 on test set using the full translation model trained for 20 Epochs\n",
    "- 0.3142 when combining both together (finetuned generation translated) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = open(\"LAMBADA/lambada_control_test_data_plain_text.txt\",\"r+\",encoding=\"utf-8\")\n",
    "lam = lam.read()\n",
    "lam = lam.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from utility import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetuned(path):\n",
    "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "    model.load_state_dict(torch.load(\"../transformers/examples/model_save/\" + str(path)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "lamT = []\n",
    "for x in lam:\n",
    "    lamT.append(tokenizer.encode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a917d4af28fb4f35a4fee5926f624fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=548118077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(GPT_org(torch.tensor(x[:-1]))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3548\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal finetune\n",
    "\n",
    "- 20 Epochs\n",
    "- 1 Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_classic_finetune/checkpoint-41599/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(model(torch.tensor(x[:-1]).to(\"cuda\"))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds_fine_1.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream_classic_finetune/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "count = 0\n",
    "for x in lamT:\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    prediction = tokenizer.decode(torch.argmax(model(torch.tensor(x[:-1]).to(\"cuda\"))[0][-1:,:]).tolist())\n",
    "    real = tokenizer.decode(x[-1])\n",
    "    results = (prediction,real)\n",
    "    predictions.append(results)\n",
    "pickle.dump(predictions,open(\"saves/save_preds_fine_20.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds_fine_1.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.325\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pickle.load(open(\"saves/save_preds_fine_20.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3264\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for occasion in predictions:\n",
    "    if occasion[0] == occasion[1]:\n",
    "        counter+=1\n",
    "print(counter/len(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translation model fully trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"The Technology Report empowers or enlightens. ==== The Technology Report empowers or enlightens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pickle.load(open(\"saves/save_preds.p\",\"rb\"))\n",
    "step1_corpus = lamT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out= []\n",
    "for x in range(len(step1)):\n",
    "    final = step1[x][0]\n",
    "    sentence = tokenizer.decode(lamT[x][:-1])\n",
    "    out.append(sentence + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = split_eos(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "for x in eos: \n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_trans.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pickle.load(open(\"saves/save_preds_trans.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "real = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in prediction:\n",
    "    out.append(x.replace(\"<|endoftext|>\",\"\"))\n",
    "prediction = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred = []\n",
    "for x in prediction: \n",
    "    pred.append(tokenizer.decode(tokenizer.encode(x)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3418\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in range(len(pred)):\n",
    "    if pred[x] == real[x][1]:\n",
    "        counter+=1\n",
    "print(counter/len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "GPT_org = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = \"The Technology Report empowers or enlightens. ==== The Technology Report empowers or enlightens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = pickle.load(open(\"saves/save_preds_fine_1.p\",\"rb\"))\n",
    "step1_corpus = lamT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "out= []\n",
    "for x in range(len(step1)):\n",
    "    final = step1[x][0]\n",
    "    sentence = tokenizer.decode(lamT[x][:-1])\n",
    "    out.append(sentence + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos = split_eos(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.load_state_dict(torch.load(\"trained_models/the_dream/pytorch_model.bin\"))\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = []\n",
    "for x in eos: \n",
    "    inp = condition + \" \" + x[-1] + \"====\"\n",
    "    inp = tokenizer.encode(inp) \n",
    "    predictions.append(generation(model,tokenizer,inp))\n",
    "    \n",
    "pickle.dump(predictions,open(\"saves/save_preds_combined.p\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = pickle.load(open(\"saves/save_preds_combined.p\",\"rb\"))\n",
    "real = pickle.load(open(\"saves/save_preds.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in prediction:\n",
    "    out.append(x.replace(\"<|endoftext|>\",\"\"))\n",
    "prediction = out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "pred = []\n",
    "for x in prediction: \n",
    "    pred.append(tokenizer.decode(tokenizer.encode(x)[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.314\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in range(len(pred)):\n",
    "    if pred[x] == real[x][1]:\n",
    "        counter+=1\n",
    "print(counter/len(pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
