{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build datasets from the raw dataset\n",
    "\n",
    "### Datasets:\n",
    "    - 4 Test sets  ✔\n",
    "    - Train set ✔\n",
    "    - Subset of train for further evaluation✔\n",
    "    \n",
    "A number of steps is performed to reach the final dataset. Since the datasets are quite big, the process is split into multiple steps and intermediate results are saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To rebuild: \n",
    "    1. download the gptdataset (small-117M) and place it in the project folder\n",
    "    2. Run through this notebook \n",
    "    3. Delete / use different checkpoints during the process to work efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import torch\n",
    "import re\n",
    "import operator\n",
    "from utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the original data into single sentences and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i, line in enumerate(open(\"original_data/small-117M.\" + str(\"test\") +\".jsonl\")):\n",
    "    texts.append(json.loads(line)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = split_eos(texts)\n",
    "final = [item for sublist in splitted for item in sublist]\n",
    "final = \"<|endoftext|>\".join(final)\n",
    "f = open(\"saves/splitOnEosDataset_v2_test.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(final)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16337865"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for i, line in enumerate(open(\"original_data/small-117M.\" + str(\"train\") +\".jsonl\")):\n",
    "    texts.append(json.loads(line)['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted = split_eos(texts)\n",
    "final = [item for sublist in splitted for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5106654"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1021330\n",
      "1021330\n",
      "1021330\n",
      "1021330\n",
      "1021330\n",
      "1021330\n"
     ]
    }
   ],
   "source": [
    "# split for better prcessing later on\n",
    "a =int(len(final)/5)\n",
    "print(a)\n",
    "for x in range(5):\n",
    "    finalSplit = final[a*x:a*(x+1)] \n",
    "    print(len(finalSplit))\n",
    "    finalSplit = \"<|endoftext|>\".join(finalSplit)\n",
    "    \n",
    "    f = open(\"saves/splitOnEosDataset_v2_\" + str(x+1) + \".txt\", \"w\",encoding = \"UTF-8\")\n",
    "    f.write(finalSplit)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct and collect stats  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next step is scriptified. run grammar_parser.py for every part of the saved data (named: EOS_corrected_v2_(nr),EOS_stats_v2_(nr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Frequency Stats: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for x in range (5):\n",
    "    stats = pickle.load(open(\"saves/EOS_stats_v2_\" + str(x+1) + \".p\", \"rb\")) \n",
    "    freq = build_frequency_stats(stats)\n",
    "    pickle.dump(freq,open(\"saves/EOS_freq_v2_\" + str(x+1) + \".p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Filter trash / correct sentences / long sentences (4 datasets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = open(\"saves/splitOnEosDataset_v2_test.txt\",encoding=\"UTF-8\")\n",
    "inp1 = inp1.read()\n",
    "inp1 = inp1.split(\"<|endoftext|>\")\n",
    "inp2 = pickle.load( open( \"saves/EOS_corrected_v2_test.p\", \"rb\" ))\n",
    "inp3 = pickle.load( open( \"saves/EOS_stats_v2_test .p\", \"rb\" ))\n",
    "stats = pickle.load( open( \"saves/EOS_freq_v2_test.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp2 = add_correct_tokens(inp2,inp3[5],len(inp1))\n",
    "sentences = (inp1,inp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103127"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The second test sets only include wrong sentence. When we want to finally test the performance in the wild, \n",
    "#we need to have wrong and correct. so a new DS is builded with both filter 3 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 were deleted since they had more than99 mistakes\n",
      "42004 sentences had no grammar mistakes.\n"
     ]
    }
   ],
   "source": [
    "filtered1 = filter_trash_3(sentences,stats[-1],99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103104"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 were deleted since they had more than1000 mistakes\n",
      "42004 sentences had no grammar mistakes. They were deleted from the dataset\n"
     ]
    }
   ],
   "source": [
    "filtered2 = filter_trash_2(sentences,stats[-1],1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Format: All sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in range (len(filtered1[0])):\n",
    "    out.append(filtered1[0][x] + \"==== \" + filtered1[1][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFiltered = []\n",
    "for x in out:\n",
    "    if len(x)>700:\n",
    "        pass\n",
    "    else:\n",
    "        outFiltered.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = out\n",
    "for x in range(len(final)):\n",
    "    if x%2 == 0: \n",
    "        final[x] = final[x].replace(\"<|endoftext|>\",\"\")\n",
    "final = \" \".join(final)\n",
    "f = open(\"build_data/EOS_new_no_filter_long.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(final)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = outFiltered\n",
    "for x in range(len(final)):\n",
    "    if x%2 == 0: \n",
    "        final[x] = final[x].replace(\"<|endoftext|>\",\"\")\n",
    "final = \" \".join(final)\n",
    "f = open(\"build_data/EOS_new_no_filter_700.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(final)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test format: Only wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for x in range (len(filtered2[0])):\n",
    "    out.append(filtered2[0][x] + \"==== \" + filtered2[1][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFiltered = []\n",
    "for x in out:\n",
    "    if len(x)>700:\n",
    "        pass\n",
    "    else:\n",
    "        outFiltered.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = out\n",
    "for x in range(len(final)):\n",
    "    if x%2 == 0: \n",
    "        final[x] = final[x].replace(\"<|endoftext|>\",\"\")\n",
    "final = \" \".join(final)\n",
    "f = open(\"build_data/EOS_new_filter_long.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(final)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = outFiltered\n",
    "for x in range(len(final)):\n",
    "    if x%2 == 0: \n",
    "        final[x] = final[x].replace(\"<|endoftext|>\",\"\")\n",
    "final = \" \".join(final)\n",
    "f = open(\"build_data/EOS_new_filter_700.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(final)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the full dataset (requires a lot of RAM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1L = []\n",
    "inp2L = []\n",
    "inp3L = []\n",
    "inpSL = []\n",
    "for x in range(5):\n",
    "    inp1 = open(\"saves//splitOnEosDataset_v2_\" +  str(x+1) + \".txt\",encoding=\"UTF-8\")\n",
    "    inp1 = inp1.read()\n",
    "    inp1L.append(inp1.split(\"<|endoftext|>\"))\n",
    "    inp2L.append(pickle.load( open( \"saves/manual_dataset/EOS_corrected_v2_\" +  str(x+1) + \".p\", \"rb\" )))\n",
    "    inp3L.append(pickle.load( open( \"saves/manual_dataset/EOS_stats_v2_\" +  str(x+1) + \".p\", \"rb\" )))\n",
    "    inpSL.append(pickle.load( open( \"saves/manual_dataset/EOS_freq_v2_\" +  str(x+1) + \".p\", \"rb\" )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for x in range(len(inp2L)):\n",
    "    print(x)\n",
    "    inp2L[x] = add_correct_tokens(inp2L[x],inp3L[x][5],len(inp1L[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for x in range(5):\n",
    "    sentences.append((inp1L[x],inp2L[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 were deleted since they had more than100 mistakes\n",
      "416139 sentences had no grammar mistakes. They were deleted from the dataset\n",
      "249 were deleted since they had more than100 mistakes\n",
      "413842 sentences had no grammar mistakes. They were deleted from the dataset\n",
      "272 were deleted since they had more than100 mistakes\n",
      "412081 sentences had no grammar mistakes. They were deleted from the dataset\n",
      "259 were deleted since they had more than100 mistakes\n",
      "413714 sentences had no grammar mistakes. They were deleted from the dataset\n",
      "265 were deleted since they had more than100 mistakes\n",
      "414202 sentences had no grammar mistakes. They were deleted from the dataset\n"
     ]
    }
   ],
   "source": [
    "final = []\n",
    "for x in range(5):\n",
    "    final.append(filter_trash_3(sentences[x],inpSL[x][-1],100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = []\n",
    "for y in range(5):\n",
    "    for x in range (len(final[y][0])):\n",
    "        out.append(final[y][0][x] + \"==== \" + final[y][1][x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "outFiltered = []\n",
    "for x in out:\n",
    "    if len(x)>700:\n",
    "        pass\n",
    "    else:\n",
    "        outFiltered.append(x)\n",
    "out = outFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(len(out)):\n",
    "    if x%2 == 0: \n",
    "        out[x] = out[x].replace(\"<|endoftext|>\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train = \" \".join(final_train)\n",
    "f = open(\"EOS_new_full_train.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(final_train)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare ~1% train for later testing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataR = open(\"EOS_new_full_train.txt\", \"r\",encoding = \"UTF-8\")\n",
    "dataR = dataR.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = str(dataR).split(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneP = data[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneP = \"<|endoftext|>\".join(oneP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"EOS_new_full_train_5k.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(oneP)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the dataset for classic finetuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First step: Correct the raw data with grammar_parser_json.py (the original data was split in two parts to improve handability) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor1 = pickle.load( open( \"gpt2-dataset/correctedtrain100k.p\", \"rb\" ))\n",
    "cor2 = pickle.load( open( \"gpt2-dataset/correctedtrain150k.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor1 = cor1 + cor2\n",
    "print(len(cor1))\n",
    "cor1 = \"\".join(cor1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"classic_finetune_train.txt\", \"w\",encoding = \"UTF-8\")\n",
    "f.write(cor1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trf",
   "language": "python",
   "name": "trf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
